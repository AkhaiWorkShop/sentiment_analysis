{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784209b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (1815, 3)\n",
      "sentimen\n",
      "positif    612\n",
      "netral     607\n",
      "negatif    596\n",
      "Name: count, dtype: int64\n",
      "Classes: ['negatif' 'netral' 'positif']\n",
      "Sizes -> train: (1270,), val: (272,), test: (273,)\n",
      "\n",
      "Data Tokenization done for IndoBERT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IndoBERT Model Architecture:\n",
      "Model: \"tf_bert_for_sequence_classification_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  124441344 \n",
      "                                                                 \n",
      " dropout_303 (Dropout)       multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2307      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124443651 (474.71 MB)\n",
      "Trainable params: 124443651 (474.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting IndoBERT Fine-Tuning...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EarlyStopping' object has no attribute '_implements_train_batch_hooks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting IndoBERT Fine-Tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Input ke model adalah dictionary (input_ids, attention_mask)\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# BERT fine-tuning hanya butuh sedikit epochs\u001b[39;49;00m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    117\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-Tuning finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# ==== 9️⃣ Evaluate ====\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/AI-ForIndonesia/sentiment_analysts/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1213\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1212\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/AI-ForIndonesia/sentiment_analysts/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/dev/AI-ForIndonesia/sentiment_analysts/.venv/lib/python3.10/site-packages/tf_keras/src/callbacks.py:245\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Performance optimization: determines if batch hooks need to be called.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_tf_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(cb, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_supports_tf_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_hooks_support_tf_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(cb, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_supports_tf_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_implements_train_batch_hooks\u001b[49m()\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m cb\u001b[38;5;241m.\u001b[39m_implements_test_batch_hooks()\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m cb\u001b[38;5;241m.\u001b[39m_implements_predict_batch_hooks()\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    251\u001b[0m     cb\u001b[38;5;241m.\u001b[39m_implements_train_batch_hooks() \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[1;32m    252\u001b[0m )\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_test_batch_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    254\u001b[0m     cb\u001b[38;5;241m.\u001b[39m_implements_test_batch_hooks() \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[1;32m    255\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EarlyStopping' object has no attribute '_implements_train_batch_hooks'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers.optimization_tf import AdamWeightDecay # ADDED: AdamW recommended for BERT fine-tuning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Konfigurasi TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# ==== IndoBERT Model Configuration ====\n",
    "# Kita gunakan IndoBERT base yang umum diakses publik\n",
    "# FIX: Kembali ke indobenchmark/indobert-base-p1\n",
    "MODEL_NAME = \"indobenchmark/indobert-base-p1\"\n",
    "MAX_LEN = 60 # Panjang sequence tetap 60\n",
    "NUM_LABELS = 3\n",
    "\n",
    "# ==== 1️⃣ Load Data ====\n",
    "try:\n",
    "    # Ganti path ini jika file berada di lokasi lain\n",
    "    df = pd.read_csv(\"../data/dataset_clean.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"WARNING: 'dataset_clean.csv' not found. Creating dummy data.\")\n",
    "    data = {\n",
    "        'clean_tweet': [\n",
    "            \"pelayanan sangat buruk sekali\", \"produk ini lumayan lah\", \"sangat bagus sekali puas\",\n",
    "            \"netral saja tidak ada komentar\", \"parah banget kecewa\", \"ini oke punya\"\n",
    "        ] * 300,\n",
    "        'sentimen': (['negatif'] * 300) + (['netral'] * 300) + (['positif'] * 300)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Loaded dataset: {df.shape}\")\n",
    "print(df['sentimen'].value_counts())\n",
    "\n",
    "# ==== 2️⃣ Encode Label ====\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['sentimen'])\n",
    "classes = label_encoder.classes_\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "# ==== 3️⃣ Split Data ====\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df['clean_tweet'], df['label'], test_size=0.3, random_state=42, stratify=df['sentimen']\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "# Konversi y ke numpy array (wajib untuk TF)\n",
    "y_train, y_val, y_test = y_train.values, y_val.values, y_test.values\n",
    "\n",
    "print(f\"Sizes -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# ==== 4️⃣ Tokenize menggunakan IndoBERT Tokenizer ====\n",
    "# IndoBERT menggunakan tokenizer khusus\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_data(texts):\n",
    "    \"\"\"Tokenisasi teks menggunakan tokenizer IndoBERT.\"\"\"\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='tf' # Mengembalikan tensor TensorFlow\n",
    "    )\n",
    "\n",
    "# Tokenisasi semua set data\n",
    "X_train_enc = tokenize_data(X_train)\n",
    "X_val_enc = tokenize_data(X_val)\n",
    "X_test_enc = tokenize_data(X_test)\n",
    "\n",
    "print(\"\\nData Tokenization done for IndoBERT.\")\n",
    "\n",
    "# ==== 5️⃣ Load Model IndoBERT untuk Klasifikasi (FIXED) ====\n",
    "# TFAutoModelForSequenceClassification otomatis menambahkan lapisan output (Dense) di atas BERT\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=NUM_LABELS,\n",
    "    # from_pt=True dihapus untuk menghindari error kerentanan PyTorch\n",
    ")\n",
    "\n",
    "# Tampilkan ringkasan model (ini akan jauh lebih besar!)\n",
    "print(\"\\nIndoBERT Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# ==== 6️⃣ Compile ====\n",
    "# Optimizer yang disarankan untuk fine-tuning BERT (AdamW)\n",
    "# Menggunakan AdamWeightDecay dari transformers untuk menghindari error serialisasi Keras/TF\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01) # AdamW\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # Gunakan from_logits=True karena model BERT tidak memiliki Softmax\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# ==== 7️⃣ Callbacks (DIHAPUS UNTUK MEMPERBAIKI ERROR) ====\n",
    "# Konflik internal antara transformers dan tf.keras.callbacks dihindari dengan menghapus semua callbacks.\n",
    "# Kita akan menggunakan epoch yang sangat sedikit (5) untuk mencegah overfitting, yang merupakan praktik standar BERT fine-tuning.\n",
    "\n",
    "# ==== 8️⃣ Train (Fine-Tuning) ====\n",
    "print(\"\\nStarting IndoBERT Fine-Tuning...\")\n",
    "# Input ke model adalah dictionary (input_ids, attention_mask)\n",
    "history = model.fit(\n",
    "    X_train_enc, y_train,\n",
    "    validation_data=(X_val_enc, y_val),\n",
    "    epochs=5, # Dikurangi menjadi 5 epochs untuk mencegah overfitting tanpa EarlyStopping\n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")\n",
    "print(\"Fine-Tuning finished.\")\n",
    "\n",
    "# ==== 9️⃣ Evaluate ====\n",
    "# Evaluasi menggunakan bobot terakhir setelah 5 epoch.\n",
    "\n",
    "# Prediksi: output model BERT adalah logits, harus diubah ke probabilitas (softmax)\n",
    "# dan kemudian di-argmax untuk mendapatkan label\n",
    "logits = model.predict(X_test_enc, verbose=0).logits\n",
    "y_pred = np.argmax(logits, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report (Test Data - IndoBERT):\")\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "\n",
    "# ==== 📊 Plot Curves ====\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs (IndoBERT)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs (IndoBERT)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
